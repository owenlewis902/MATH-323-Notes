\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}

\graphicspath{{pics/}}

%CURLIES  :)       $\{$ $\}$

\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\Om}{\Omega}
\newcommand{\om}{\omega}
\newcommand{\la}{\lambda}
\newcommand{\ep}{\varepsilon}
\newcommand{\emp}{\emptyset}
\newcommand{\lt}{\textless}
\newcommand{\gt}{\textgreater}
\newcommand{\imply}{\Rightarrow}
\newcommand{\x}{\cdot}

\title{MATH 323 Class Notes}
\author{Owen Lewis}
\date{Summer 2018}

\begin{document}
\begin{titlepage}
\maketitle
\end{titlepage}

\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 1, 2018}
\subsection{Definitions}
Let $\Omega$ be the set of all possible outcomes. We call $\Omega$ the $\ti{Sample Space}$.\\\\
\textbf{Ex 1:} Flipping a coin. The possible outcomes are $H$ and $T$\\
$\imply$ $\Omega$ = $\{$$H$, $T$$\}$.\\
\textbf{Ex 2:} Tossing a die. We list all the outcomes as $\omega_{i}$ where $i$ is the face of the die that we land on. We'll assume a normal 6-sided die\\ 
$\imply$ $\Omega$ = $\{$$\omega_{1}$, $\omega_{2}$,\dots, $\omega_{6}$$\}$.\\
\textbf{Ex 3:} Flipping a coin until an $H$ appears. The possible outcomes are $H$, $TH$, $TTH$,\dots, $TT$$\dots$$TH$ (with $n-1$ $T$s), $\dots$ ad infinitum.\\\\
$\imply$ $\Omega$ = $\{$$H, TH, TTH, TTTH, TTTTH,\dots$$\}$.\\
Note that in this case, $\Omega$ is a (countably) infinite set!\\\\
Let $\Omega$ be a sample space. Any subset $A$ of $\Omega$ is called an $\it{Event}$.
\begin{itemize}
	\item If $A$ = $\emptyset$, then we call $A$ the $\ti{Null Event}$
	\item If $A$ = $\Omega$, then we call $A$ the $\ti{Certain Event}$
	\item If $|$$A$$|$ = 1, then we call $A$ an $\ti{Elementary Event}$
\end{itemize}
\textbf{Ex:} Tossing a die. Let $\Omega$ := $\{$$\omega_{1}$, $\omega_{2}$,\dots, $\omega_{6}$$\}$, $A$ := $\{$$\omega_{1}$, $\omega_{2}$$\}$. Then $A$ is an event but is not an elementary event.\\\\
If $A$ is an event, then $A^{c}$ is also an event called the $\ti{complement event}$ of $A$.\\
If $A, B$ are two disjoint events then we call $A$, and $B$ $\ti{mutually exclusive}$, or $\ti{disjoint}$.\\\\
Let $\Omega$ be a sample space, $\mathcal{P}$ be the power set of $\Om$. A $\ti{Probability}$ $\bbP$ on $\Om$ is a function $\bbP$ : $\mathcal{P}$($\Om$) $\rightarrow$ [0, 1], such that:
\begin{enumerate}
	\item $\forall$ $A$ $\subseteq$ $\Om$, 0 $\leq$ $\bbP$($A$) $\leq$ 1
	\item $\bbP$($\Om$) = 1
	\item If $A_{1}$, $A_{2}$,\dots, $A_{n}$,\dots\ is a sequence of pairwise disjoint events then
\[ \bbP(\bigcup_{i=1}^{\infty} p_{i}) = \sum_{i=0}^{\infty} \bbP(A_{i}) \]
\end{enumerate}
\newpage
\subsection{How do we Apply this?}
Let $\Om$ be a discrete set, $E_{i}$ = $\{$$\omega_{i}$$\}$ be an elemental event, with $E_{i}$ $\subseteq$ $\Om$. A probability on $\Om$ is given by a sequence $\bbP_{1}$, $\bbP_{2}$,\dots, $\bbP_{n}$,\dots of positive numbers such that
\[ \bbP(E_{i}) = p_{i},\ and\ \sum_{i} \bbP(p_{i}) = 1\]\\
If $A$ $\subseteq$ $\Om$, then
\[ \bbP(A) = \sum_{\omega_{i} \in A} \bbP(p_{i})\]\\
\textbf{Ex 1:} Toss a die.\\
a) Given that $\bbP$($\omega_{2}$) = $\bbP$($\omega_{4}$) = $\bbP$($\omega_{5}$) = $\bbP$($\omega_{6}$) = $\frac{1}{6}$, $\bbP$($\omega_{1}$) = $\frac{1}{4}$, find $\bbP$($\omega_{3}$).\\
b) Find the probability that the die will land on an odd face.\\
\textbf{Solution:}\\
a) $\Omega$ = $\{$$\omega_{1}$, $\omega_{2}$,\dots, $\omega_{6}$$\}$. $\bbP$($\omega_{3}$) is a singleton, and as we know $\sum_{i} \bbP(p_{i})$ = 1 then  $\sum_{i=1}^{6}\bbP(\omega_{i})$ = 1 $\implies$ $\bbP$($\omega_{2}$) + $\bbP$($\omega_{4}$) + $\bbP$($\omega_{5}$) + $\bbP$($\omega_{6}$) + $\bbP$($\omega_{1}$) + $\bbP$($\omega_{3}$) = 1 $\implies$ $\frac{4}{6}$ + $\frac{1}{4}$ + $\bbP$($\omega_{3}$) = 1 $\implies$ $\bbP$($\omega_{3}$) = 1 - $\frac{11}{12}$\\
$\implies$ $\bbP$($\omega_{3}$) = $\frac{1}{12}$.\\
b) Let $A$ $\subseteq$ $\Om$ be the subset of $\Om$ containing all the odd faces. The total probablilty of $A$ is therefore the sum of all the probabilities of the elements of $A$ $\imply$ $\bbP$($A_{i}$) = $\frac{1}{4}$ + $\frac{1}{12}$ + $\frac{1}{6}$ = $\frac{1}{2}$.\\\\
\textbf{Ex 2:} Given a countably infinite sample space, find a constant $c$ such that $\bbP$($\{$$\omega_{n}$$\}$) = $c(\frac{1}{5})^n$ for some $n$.\\
\textbf{Solution:} $\sum_{n=1}^{\infty} \bbP$($\{$$\omega_{n}$$\}$) = 1 $\imply$ $\sum_{n=1}^{\infty} c(\frac{1}{5})^n$ = 1 $\imply$ $\sum_{n=1}^{\infty} (\frac{1}{5})^n$ = $\frac{1}{c}$. Notice how $\sum_{n=1}^{\infty} (\frac{1}{5})^n$ is a geometric series that converges to $\frac{1}{4}$ $\imply$ therefore $\frac{1}{c}$ = $\frac{1}{4}$\\
$\imply$ $c$ = 4.
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 2, 2018}
\subsection{Properties of $\bbP$}
Let $\Om$ be a sample space and let $\bbP$ be a probability on $\Om$. Then:
\begin{enumerate}
	\item $\bbP$($\emptyset$) = 0
	\item $\bbP$($A^{c}$) = 1 - $\bbP$($A$)
	\item $\bbP$($A \cup B$) = $\bbP(A)$ + $\bbP(B)$ - $\bbP(A \cap B)$
\end{enumerate}
Proof:
\begin{enumerate}
	\item Set $A_{1}$ = $\Om$, $A_{2}$ = $\emp$. Then $A_{1} \cup A_{2}$ = $\Om$, and $A_{1} \cap A_{2}$ = $\emp$. Therefore $\bbP(A_{1} \cup A_{2}$) = $\bbP(A_{1})$ + $\bbP(A_{2})$ $\imply$ $\bbP(\Om)$ = $\bbP(\emp)$ + $\bbP(\Om)$ $\imply$ 1 = $\bbP(\emp)$ + 1\\ $\imply$ 0 = $\bbP(\emp)$. \qed

	\item $A^{c} \cup A$ = $\Om$, and $A^{c} \cap A$ = $\emp$. Then = $\bbP(\Om)$ $\imply$ 1 = $\bbP(A^{c} \cup A)$\\ $\imply$ 1 = $\bbP(A)$ + $\bbP(A^{c})$ $\imply$ $\bbP(A^{c})$ = 1 - $\bbP(A)$. \qed

	\item It's easy to show that $A$ = $(A \setminus B) \cup (A \cap B)$, and $\emp$ = $(A \setminus B) \cap (A \cap B)$. Similarily, $\bbP(A)$ = $\bbP(A \setminus B)$ + $\bbP(A \cap B)$. From these, it follows that\\ $A \cup B$ = $A \cup (B \setminus A)$ $\imply$ $\bbP(A \cup B)$ = $\bbP(A)$ + $\bbP(B \setminus A)$\\ $\imply$ $\bbP$($A \cup B$) = $\bbP(A)$ + $\bbP(B)$ - $\bbP(A \cap B)$. \qed
\end{enumerate}

\subsection{Equiprobability}
Let $\Om$ be a finite sample space. Set $N$ := $|\Om|$. Equiprobability means that all outcomes have the same probability $\bbP$ = $\frac{1}{N}$. Let $A \subseteq \Om$ be an event. Then we have $\bbP(A)$ = $|A|$$\cdot$$\frac{1}{N}$, or 
\[\bbP(A) = \frac{|A|}{|\Om|}\]
This is great because it means that in \textbf{equiprobability} problems we just need to count the cardinality of $A$, count the cardinality of $\Om$, and divide them, and we're done. Too bad counting isn't really all that easy.

\subsection{Counting Tools}
We have here three tools to help us calculate the cardinalities of huge (finite) subsets of huge sample spaces, each with their own specific situations that require its use
\begin{enumerate}
	\item The Cartesian Product
	\item Permutations
	\item Combinations
\end{enumerate}
\subsubsection{The Cartesian Product}
\paragraph{}
We all know what the cartesian product $is$. In probability we use it in our experiment when we have more than one """input""" each with its own possible outcome, for example, we roll three dice, or flip two coins.
\paragraph{}
Let $A$, $B$, be sets such that $|A|$ = $a$, and $|B|$ = $b$. Then the cardinality of the cartesian product $|(A \times B)|$ is $|(A \times B)|$ = $a \cdot b$.\\\\
\textbf{Ex 1:} Suppose we roll a die twice. What is the cardinality of the sample space $\Om$?\\
\textbf{Solution:} If we roll a die once we have $\Om$ = $\{$$\omega_{1}$, \dots, $\omega_{6}$$\}$. Therefore, the sample space for rolling a die twice is $\Om \times \Om$. The cardinality of our sample space $\Om \times \Om$ is $|\Om \times \Om|$ = 6 $\cdot$ 6 = 36.\\\\
\textbf{Ex 2:} Suppose we roll a fair die twice. What is the probability that the outcome is even?\\
\textbf{Solution:} If the sum of the two outcomes is even then both must either be even or both must be odd. Let $A$ be the event where the sum of the rolls is even, and let $A_{1}$ be event where both individual rolls are even, $A_{2}$ be the event where both individual rolls are odd. For example,
\[A_{1} = \{(2, 2), (2, 4), (2, 6), (4, 2), (4, 4), (4, 6), (6, 2), (6, 4), (6, 6)\}\] 
Where the 1$^{st}$ element in each ordered pair is the outcome of the 1$^{st}$ roll and the 2$^{nd}$ element in each ordered pair is the outcome of the 2$^{nd}$ roll. Therefore, we have that $|A_{1}|$ = $|A_{2}|$ = 9.\\ 
Since naturally $A$ = $A_{1} \cup A_{2}$ then $\bbP(A)$ = $\bbP(A_{1})$ + $\bbP(A_{2})$. We also know that the die was fair, so we can use our equiprobability formula here.
\[ \bbP(A) = \bbP(A_{1}) + \bbP(A_{2}) = \frac{|A_{1}|}{|\Om|} + \frac{|A_{2}|}{|\Om|} = \frac{9}{36} + \frac{9}{36} = \frac {18}{36} = \frac{1}{2}.\]
\subsubsection{Permutations}
A permutation of $r$ integer elements chosen from $n$ (possible elements) is equivalent to a successive draw, without replacement, of $r$ elements from a list of $n$ elements. We denote the number of possibilities by $P_{r}^{n}$. The general formula for $P_{r}^{n}$ is
\[P_{r}^{n} = n \cdot (n-1) \cdot \dots \cdot (n-r+1) = \frac{n!}{(n-r)!}\]\\
\textbf{Ex:} \\
a) A thick black bag contains 4 balls: 1 green, 1 blue, 1 red, 1 yellow. The bag is made of lead, or something, and also light cannot exist in this bag. You couldn't see into this bag if your life depended on it. Draw successively two balls from the bag without putting them back in. What is the probability that the second ball drawn is green?\\
b) What is the probability that one of the two balls drawn will be green?\\
\textbf{Solution:}\\
a) Let $\Om$ be the set of permutations of 2 balls chosen from the bag containing 4 balls. Then, $|P^{4}_{2}|$ = $\frac{4!}{2!}$ = $\frac{24}{2}$ = 12. Then let $A$ be the event where the second ball is green. The cardinality of $A$ is 3, as if we take for granted that the second ball is green, then there are 3 other different-coloured balls in the bag to accompany it. Since the bag is so dark that it's physically impossible to see inside it, all the balls in the bag have an equal probability of being drawn. Therefore,
\[ \bbP(A) = \frac{|A|}{|\Om|} = \frac{3}{12} = \frac{1}{4}\]
b) Let $B$ be the event where one of the balls drawn is green. Suppose $B_{1}$ is the event where the first ball drawn is green, and $B_{2}$ is the event where the second ball drawn is green. In part a) we found that $|B_{2}|$ = 3, and a parallel argument shows that $|B_{2}|$ = 3 as well. Thus,
 \[ \bbP(B) = \frac{|B_{1}|}{|\Om|} + \frac{|B_{2}|}{|\Om|} = \frac{3}{12} + \frac{3}{12} = \frac{6}{12} = \frac{1}{2}\]
\subsubsection{Combinations}
Consider a set $\Om$ with $n$ elements. Let $r$ be an integer such that $0 \leq r \leq n$. A combination $C^{n}_{r}$, also denoted ${n}\choose{r}$ (pronounced $n$ choose $r$), is the number of subsets of $\Om$ containing $r$ elements. The general formula is
\[C^{n}_{r} = \frac{P_{n}^{r}}{r!} = \frac{n!}{r! \cdot (n-r)!}\]
\textbf{Ex:}\\
a) Out of a deck of 52 cards how many distinct 5-card hands are possible?\\
b) What is the probability that a given hand contains at least one ace?\\
\textbf{Solution:}\\
a) ${52}\choose{5}$ = $\frac{52!}{5! \cdot (47!)}$ = 2,598,960\\
b) In this case it is easier to calculate the probability where the hand contains no aces and then subtract that from 1 to find the probability that we have an ace. If $A$ is the event where the hand contains an ace, then $A^{c}$ is the event where a hand contains no ace. Then
\[\bbP(A^{c}) =  \frac{|A^{c}|}{|\Om|} = \frac{C^{48}_{5}}{C^{52}_{5}} = \frac{1,712,304}{2,598,960} = \frac{35,673}{54,145}\]
Then we need to subtract this from 1 and we're done
\[\bbP(A) = 1 - \frac{35,673}{54,145} \approx 0.3412\]
\subsection{Properties of Combinations}
Here are some properties of combinations
\begin{enumerate}
	\item $C^{n}_{0}$ = $C^{n}_{n}$ = 1
	\item $C^{n}_{r}$ = $C^{n}_{n-r}$ $0 \leq r \leq n$
	\item $C^{n}_{r}$ = $C^{n-1}_{r}$ + $C^{n-1}_{r-1}$ $0 \leq r \leq n$
\end{enumerate}
The proofs are really easy and I dont wan't to bother typing them out but here is the idea behind each of them:
\begin{enumerate}
	\item Trivial
	\item Induction on $n$
	\item Also induction on $n$
\end{enumerate}
\subsubsection{Pascal's Triangle}
Pascal's triangle is a table of combinations $C_{r}^{n}$. The rows of the triangle represent $n$, starting at 0 at the tip and working down, and the $r^{th}$ element from the left (starting at 0) represents $r$. Each number in the triangle is determined by summing the two numbers directly above it.\\
\begin{center}
\includegraphics[scale=0.5]{pascal.png}\\
\textit{Pascal's Triangle}
\end{center}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 3, 2018}
\subsection{Binomial Theorem}
$(a+b)^{n}$ = $\sum_{k=0}^{n} C_{k}^{n} \cdot a^{k} \cdot b^{n-k}$\\\\
\textbf{Ex 1:} $(a+b)^{3}$ = $C_{0}^{3} \cdot a^{6}b^{3}$ + $C_{1}^{3} \cdot ab^{2}$ + $C_{2}^{3} \cdot a^{2}b$ + $C_{3}^{3} \cdot a^{3}b$ = $b^{3} + 3ab^{2} + 3ba^{3} + a^{3}$\\\\
\textbf{Ex 2:} Find the coefficient of $x^{6}$ in the expansion of $(x^{2}+2)^{7}$.\\
\textbf{Solution:} If we let $x^{2}$ := $a$ and 2 := $b$, then from the binomial theorem:
\[(a+b)^{n} = \sum_{k=0}^{n} C_{k}^{n} \cdot a^{k} \cdot b^{n-k} = \sum_{k=0}^{n} C_{k}^{7} \cdot a^{k} \cdot b^{7-k} = C_{3}^{7} \cdot 2^{4} = 560\]
\subsection{Conditional Probability}
Let $\Om$ be a sample space and let $\bbP$ be a probability on $\Om$. Let $A \subseteq \Om$ be an event such that $\bbP(A)\ \gt\ 0$, and let $B \subseteq \Om$ be another event. The \textit{conditional probability of $B$ given $A$}, is defined as
\[ \bbP(A|B) = \frac{\bbP(B \cap A)}{\bbP(A)}\]\\
$\textbf{Ex 1:}$ We have 2 urns. Urn 1 contains 7 red balls and 4 blue balls, and urn 2 contains 5 red balls and 6 blue balls. First, we choose a ball from urn 1 and place it into urn 2, then we remove a ball from urn 2. What is the probability that the ball that we remove from urn 2 will be blue?\\
\textbf{Solution:} Let $B$ be the event where the ball drawn from urn 2 is blue, and let $A_{r}$, $A_{b}$ be the events where we draw a red or a blue ball from urn 1 respectively. Then $\bbP(A_{r})$ = $\frac{7}{11}$ and $\bbP(A_{b})$ = $\frac{4}{11}$. Also,
\[B = B \cap \Om \iff B = B \cap (A_{r} \cup A_{b}) \iff B = (B \cap A_{r}) \cup (B \cap A_{b})\]
Which then implies
\[\bbP(B) = \bbP(B \cap A_{r}) + \bbP(B \cap A_{b}) \iff \bbP(B) = \bbP(B|A_{r}) \cdot \bbP(A_{r}) + \bbP(B|A_{b}) \cdot \bbP(A_{b})\]
So all that is left is to compute $\bbP(B|A_{r})$, and $\bbP(B|A_{b})$.\\
From the definition, we can see that $\bbP(B|A_{r})$ = $\frac{1}{2}$, and $\bbP(B|A_{b})$ = $\frac{7}{12}$, so finally
\[ \bbP(B) = \frac{6}{12} \cdot \frac{7}{11} + \frac{7}{12} \cdot \frac{4}{11} = \frac{70}{132} \]\\
\textbf{Ex: 2} Roll a fair die. Let $A \subseteq \Om$ be the event where the outcome is even, $B \subseteq \Om$ be the event where the outcome is odd, $C \subseteq \Om$ be the event where the outcome is either a 1 or a 2. Compute all the conditional probabilities.\\
\textbf{Solution} First $\bbP(A) = \frac{1}{2}$, $\bbP(B) = \frac{1}{2}$, and $\bbP(C) = \frac{1}{3}$ Then:\\
$\bbP(A|B) = \frac{\frac{1}{6}}{\frac{1}{2}}$ = $\frac{1}{3}$, and $\bbP(B|A) = \frac{\frac{1}{6}}{\frac{1}{2}}$ = $\frac{1}{3}$\\
$\bbP(A|C) = \frac{1}{2}$, and $\bbP(C|A) = \frac{1}{3}$\\
$\bbP(B|C)$ = 1, and $\bbP(C|B) = \frac{2}{3}$\\
\subsection{Independent Events}
Let $A$, and $B$ be two events over some sample space $\Om$. $A$, and $B$ are said to be $independent$ if and only if:
\begin{enumerate}
	\item $\bbP(A|B)$ = $\bbP(A)$
	\item $\bbP(B|A)$ = $\bbP(B)$
	\item $\bbP(A \cap B)$ = $\bbP(A)$ $\cdot$ $\bbP(B)$
\end{enumerate}
\textbf{Ex: }\\ 
a) From the previous example, are $A$ and $B$ independent? \\
b) What about $A$ and $C$?\\
c) $B$ and $C$?\\
\textbf{Answers:}\\
a) No.\\
b) Yes.\\
c) No.\\
\subsection{Baye's Rule}
Let $A, B \subseteq \Om$ be events on a sample space $\Om$. Then the equation
\[ \bbP(A|B) = \frac{\bbP(B|A) \cdot \bbP(A)}{\bbP(B)}\]
is calles Baye's Rule, or Baye's Theorem.\\\\
\textbf{Theorem:} Total Probability Rule\\ 
Let $\Om$ be a sample space, and let $A \subseteq \Om$ be an event. Suppose we partition $\Om$ like $\Om$ = $\{$$B_{1}, B_{2}, \dots, B_{n}$$\}$, where the $B_{i}$s are pairwise disjoint events such that
\[ \bigcup_{i=1}^{n}B_{i} = \Om\]
Then:
\begin{enumerate}
	\item $\bbP(A)$ = $\sum_{i=1}^{n}$$\bbP(A|B_{i}) \cdot \bbP(B_{i})$, and
	\item If $k \in \N$ is fixed, then $\bbP(B_{k}|A)$ = $\frac{\bbP(A|B_{k}) \cdot \bbP(B_{k})}{\sum_{i=1}^{n}\bbP(A|B_{i}) \cdot \bbP(B_{i})}$
\end{enumerate}
\textbf{Proof:}
\begin{enumerate}
	\item We know that $A$ = $A \cap \Om$ $\iff$ $A$ = $A \cap (\bigcup_{i=1}^{n}B_{i})$ $\iff$ $A$ = $\bigcup_{i=1}^{n}A \cap B_{i}$. Therefore, $\bbP(A)$ = $\sum_{i=1}^{n}\bbP(A \cap B_{i})$ $\iff$ $\bbP(A)$ = $\sum_{i=1}^{n}\bbP(A|B_{i})\cdot \bbP(B_{i})$ \qed
	\item $\bbP(B_{k}|A)$ = $\frac{\bbP(B_{k} \cap A)}{\bbP(A)}$ = $\frac{\bbP(A|B_{k})\cdot \bbP(B_{k})}{\bbP(A)}$ = $\frac{\bbP(A|B_{k}) \cdot \bbP(B_{k})}{\sum_{i=1}^{n}\bbP(A|B_{i}) \cdot \bbP(B_{i})}$ \qed \\\\\\
\end{enumerate}
\textbf{Ex:} A (very simple) forecast model
\\Suppose that on a given day the weather is one of two states: sunny or rainy. Let $R \subseteq \Om$ be the event where it's rainy and $S \subseteq \Om$ be the event where it's sunny. If today is rainy then the probability that tomorrow will also be rainy is 60\%. On the other hand, if today is sunny then the probability that tomorrow will be sunny is 70\%.\\
a) If Monday is sunny then what is the probability that Wednesday will also be sunny?\\
b) If Wednesday is sunny then what is the probability that Tuesday was rainy?\\
\textbf{Solutions:}\\
a) Let $A$ be the event where it is sunny on Wednesday. Then
\[ \bbP(A) = \bbP(A|A_{1}) \cdot \bbP(A_{1}) + \bbP(A|A_{2}) \cdot \bbP(A_{2}) = 0.7 \cdot 0.7 + 0.4 \cdot 0.3 = 0.61\]
where $A_{1}$ is the event where it's sunny on Tuesday, and $A_{2}$ is the event where it's rainy on Tuesday.\\
b) $\bbP(A_{2}|A)$ = $\frac{\bbP(A|A_{2}) \cdot \bbP(A_{2})}{\bbP(A)}$ = $\frac{0.12}{0.61}$ = $\frac{12}{61}$

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 7, 2018}
\subsection{When to use Permutations vs Combinations}
To determine whether we need to use a permutation or a combination we first need to consider two things. First, the sampling method:
\begin{enumerate}
	\item Without Replacement
	\item With Replacement
\end{enumerate} 
and whether we care about the order of the sample points:
\begin{enumerate}
\setcounter{enumi}{2}
	\item Order doesn't matter
	\item Order matters
\end{enumerate}
We use permutations when we have 1\&4, and\\
we use combinations when we have 1\&3 $or$ 2\&4.
\subsection{Multinomial Coefficients}
The number of ways we can partition $n$ objects into $k$ distinct groups containing $n_{1}$, $n_{n}$, \dots, $n_{k}$ objects, respectively, where each object appears in exactly one group and $\sum_{i=1}^{k} n_{i}$ = $n$, is
\[N := {n\choose{n_{1}, n_{2}, \dots, n_{k}}} = \frac{n!}{n_{1}! n_{2}! \dots n_{k}!} \]
where we call
\[{n\choose{n_{1}, n_{2}, \dots, n_{k}}} \]
the \ti{multinomial coefficient}.\\\\
\textbf{Ex 1:} We wish to expand $(x + y + z)^{17}$. What is the coefficient on the $x^{2}y^{5}z^{10}$ term?\\
\textbf{Solution:} The coefficient is ${17\choose{2, 5, 10}}$ = $\frac{17!}{2!5!10!}$\\\\
\textbf{Ex 2:} \#62 from the first assignment.\\
\textbf{Solution:} Total number = ${9\choose{3, 3, 3}}$, Desired number = ${7\choose{1, 3, 3}}$.\\
Therefore, $\bbP(A)$ = $\frac{{7\choose{1, 3, 3}}}{{9\choose{3, 3, 3}}}$
\subsection{Discrete Random Variables}
\ti{Random Variables} are variables that take on random values based on the outcome of the experiment. Each random variable is associated with a \ti{probability distribution} that specifies the possible random variable values and the probability each value will occur. A random variable is said to be \ti{discrete} if it can get only a finite or countably infinite number of possible distinct values.\\\\
The probability that the random variable $Y$ will take on the value $y$, denoted by $\bbP(Y=y)$, is defined as the sum of the probabilities of all the sample points $\om \in \Om$ that are assigned the value of $y$.
\subsection{Probability Distribution Representations}
The probability distribution for a discrete random variable $Y$ can be represented as a rule (formula), a table, or a graph. \\
\textbf{Ex: }
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c |}
\hline
$y$ & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12\\
\hline
$\bbP(Y=y)$ & $\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ & $\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ & $\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$\\
\hline
\end{tabular}
\end{center}
is a probability distribution of $y$.\\\\
\underline{Theorem:} For any discrete probability distribution, the following hold:
\begin{enumerate}
	\item $\bbP(Y=y) \geq 0$, $\forall y$
	\item $\sum_{y} \bbP(Y=y)$ = 1
\end{enumerate}
\subsection{Expected Value}
Let $Y$ be a discrete random variable with a probability function $\bbP(Y=y)$. Then the expected value of $Y$, $E(Y)$, is defined to be
\[E(Y) = \sum_{y} y\cdot \bbP(Y=y).\]
The expected value exists only if the above summation is \ti{absolutely convergent}. We often denote $E(Y)$ by $\mu$.\\\\
\textbf{Ex:} Consider a random variable $Y$ with the following probability distribution.
\begin{center}
\begin{tabular}{| c || c | c | c|}
\hline
$y$ & 0 & 1 & 2\\
\hline
$\bbP(Y=y)$ & $\frac{1}{4}$ & $\frac{1}{2}$ & $\frac{1}{4}$\\
\hline
\end{tabular}
\end{center}
Then
\[\mu = \sum_{y} y\cdot \bbP(Y=y) = 0\cdot \frac{1}{4} + 1\cdot \frac{1}{2} + 2\cdot \frac{1}{4} = \frac{1}{2} + \frac{1}{2} = 1\]
\subsection{Expected Value of a Function of a Random Variable}
\underline{Theorem:} Let $Y$ be the discrete random variable with probability function $\bbP(Y=y)$, and let $g(Y)$ be a real-valued function of $Y$. Then, the expected value of $g(Y)$ is
\[E(g(Y)) = \sum_{y} g(Y)\cdot \bbP(Y=y).\]
\subsection{Varience of a Random Variable}
For a random variable $Y$ with mean $E(Y) \equiv \mu$, the \ti{varience} of $Y$, $V(Y)$, is defined as the expected value of $(Y - \mu)^{2}$. That is
\[V(Y) = E((y-\mu)^{2}).\]
The varience represents the "average squared deviation of $Y$ from its mean". Intuitively, it's a measure of how variable a random variable is. The bigger the value of the varience is, the more spread out the values that the random variable can take on are.
\subsection{Standard Deviation of a Random Variable}
The \ti{standard deviation}, $\sigma$, of a random variable $Y$, is defined as the principle square root of $V(Y)$. That is
\[\sigma^{2} = V(Y) \iff \sigma = |\sqrt{V(Y)}|\]
This is a little easier to visualize than varience.
\subsection{Some Theorems (without proof)}
\underline{Theorem:} Let $Y$ be a discrete random variable with the probability function $\bbP(Y=y)$, and let $c \in \R$ be a constant. Then
\[E(c) = c\]
\underline{Theorem:} Let $Y$ be a discrete random variable with the probability function $\bbP(Y=y)$, let $g(Y)$ be a function of $Y$, and let $c$ be a constant. Then
\[E(c\cdot g(Y)) = c\cdot E(g(Y))\]
\underline{Theorem:} Let $Y$ be a discrete random variable with the probability function $\bbP(Y=y)$, and let $g_{1}(Y), g_{2}(Y)$, \dots, $g_{k}(Y)$ be $k$ functions of $Y$. Then
\[E(g_{1}(Y) + g_{2}(Y) + \dots + g_{k}(Y)) = E(g_{1}(Y)) + E(g_{2}(Y)) + \dots + E(g_{k}(Y))\]
\underline{Theorem:} Let $Y$ be a discrete random variable with the probability function $\bbP(Y=y)$ and mean $\mu$. Then
\[V(Y) \equiv \sigma^{2} = E((Y-\mu)^{2}) = E(Y^{2})-\mu^{2}\]
\subsection{Binomial Probability Distribution}
The \ti{binomial probability distribution} is a probability distribution that comes up pretty commonly. It applies where
\begin{itemize}
	\item There's a sequence of independent or identical trials
	\item Each trial can result in one of two outcomes (flipping a coin, for example).
\end{itemize}
More precisely, a binomial experiment is such that
\begin{itemize}
	\item Consists of a fixed number of $n$ trials
	\item Each trial results in one of two possible outcomes: $S$(uccess), or $F$(ailure).
	\item The probability of $S$ on any trial is $p$, and the probability of $F$ on any trial is $q=1-p$.
	\item All trials are independent
	\item The random variable $Y$ is the number of successes out of $n$ trials.
\end{itemize}
A random variable $Y$ is said to have binomial distribution based on $n$ trials iff
\[\bbP(Y=y) = {n\choose{y}}p^{n}q^{n-y}\]

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 8, 2018}
\subsection{Bernoulli Distribution}
Let $X$ be a random variable. Toss a coin. The outcomes are $H$, and $T$. Set $X(H) = 1$, and $X(T) = 0$. We have $X(\Om) = \{0, 1\}$.\\ Assume $\bbP(H) = p$, for $0\ \lt\ p\ \lt\ 1$. Then the probability function $\bbP(X=x)$ of $X$ is
\begin{center}
\begin{tabular}{| c || c | c |}
\hline
$x$ & 0 & 1\\
\hline
$\bbP(X = x)$ & 1-p & p\\
\hline
\end{tabular}
\end{center}
A random variable $X$ that has the above probability function is called a \ti{Bernoulli random variable} on $X$. Also, $X$ is said to have the \ti{Bernoulli Distribution} with parameter $p$. We write $X \sim$ Ber($p$).\\\\
\underline{Proposition:} If $X \sim$ Ber($p$), then
\begin{enumerate}
	\item $E(X) = p$
	\item $V(X) = p(1-p)$
\end{enumerate}
\underline{Proof} of (1):\\
$E(X) = \sum_{x} x\cdot \bbP(X=x)$\ = $0\cdot \bbP(X=0) + 1\cdot \bbP(X=1) = p.$ \qed \\\\
\underline{Proof} of (2):\\
$V(X) = E(X-E(X)^{2})$ = $E(X^{2}) - (E(X))^{2}$.\\
Also, $E(X^{2})$ = $\sum_{x} x^{2}\cdot \bbP(X=x)$ = $p$.\\
Therefore, $V(X) = p-p^{2} = p(1-p)$. \qed
\subsection{Some Properties of $E$, and $V$}
Let $X$ be a random variable, and let $c, \alpha \in \R$ be constants.
\begin{enumerate}
	\item $E(X+c)=E(X)+c$
	\item $V(X+c)=V(X)$
	\item $E(\alpha X)=\alpha E(X)$
	\item $V(\alpha X)=\alpha^{2} V(X)$
\end{enumerate}
\ \\\\
\subsection{A Proof that $V(X) = E(X^{2}) - (E(X))^{2}$}
Property: If $f$, $g$ are functions, then $E(f(x) + g(x)) = E(f(x)) + E(g(x))$.\\\\
Set $\mu := E(X)$. Then, by definition,
\begin{align*}
	V(X) &= E((X-E(X))^{2})\\
		&= E(x^{2} -2\mu x + \mu^{2})\\
		&= E(X^{2}) -2\mu E(X) + \mu^{2}\\
		&= E(X^{2})-2\mu^{2} + \mu^{2}\\
		&= E(X^{2}) - \mu^{2}\\
		&= E(X^{2}) - (E(X))^{2}
\end{align*}
\qed
\subsection{More on Binomial Distribution}
\underline{Proposition:} If $X \sim$ Bin($n$, $p$), then
\begin{enumerate}
	\item $E(X) = np$
	\item $V(X) = np(1-p)$
\end{enumerate}
To prove there we first have to note that
\[k\cdot {n\choose k} = k\cdot \frac{n!}{k!(n-k)!} = n\cdot \frac{(n-1)!}{(k-1)!(n-k)!} = n\cdot {{n-1}\choose{k-1}}\]\\
\underline{Proof} of (1): By definition,
\begin{align*}
	E(X) &= \sum_{k=0}^{n} k\cdot \bbP(X=k)\\
		&= \sum_{k=0}^{n} k\cdot {n\choose k}\cdot p^{k}(1-p)^{n-k}\\
		&= \sum_{k=0}^{n} n\cdot {{n-1}\choose{k-1}}\cdot p^{k}(1-p)^{n-k}\\
		\intertext{Let $\ell = k-1$}
		&= n\cdot \sum_{\ell = 0}^{n-1} {{n-1}\choose{\ell}}\cdot p^{\ell + 1}(1-p)^{(n-1)-\ell}\\
		&= np\cdot \sum_{\ell=0}^{n-1} {{n-1}\choose{\ell}}\cdot p^{\ell}(1-p)^{(n-1)-\ell}\\
		&= np(p+(1-p))^{n-1}\\
		&= np
\end{align*}
\qed\\\\
\underline{Proof} of (2): By definition, 
\begin{align*}
	V(X) &= E(X^{2}) - n^{2}p^{2}\\
		&= E(X(X-1)+X) - n^{2}p^{2}\\
		&= E(X(X-1))+np-n^{2}p^{2}\\
\end{align*}
Now we need to reduce $E(X(X-1))$  down.
\begin{align*}
	E(X(X-1)) &= \sum_{k=0}^{n} k\cdot (k-1)\cdot {n\choose k}\cdot p^{k}(1-p)^{n-k}\\
			&= \sum_{k=2}^{n} k\cdot (k-1)\cdot {n\choose k}\cdot p^{k}(1-p)^{n-k}
\end{align*}
Now we need to change the variables in $k(k-1)\cdot {n\choose k}$.
\[k\cdot (k-1)\cdot {n\choose k} = n\cdot (n-1)\cdot {{n-2}\choose{k-2}}\]
so now plugging that back into $E(X)$ we get
\begin{align*}
	E(X(X-1)) &= \sum_{k=2}^{n} k\cdot (k-1)\cdot {n\choose k}\cdot p^{k}(1-p)^{n-k}\\
			&= \sum_{k=2}^{n} n\cdot (n-1)\cdot {{n-2}\choose{k-2}}\cdot p^{k}(1-p)^{n-k}
			\intertext{Let $\ell = k-2$}
			&= n\cdot (n-1)\cdot \sum_{\ell =0}^{n-2} {{n-2}\choose{\ell}}\cdot p^{\ell}(1-p)^{(n-2)-\ell}\\
			&= n\cdot(n-1)\cdot p^{2}\cdot (p + (1-p))^{n-2}\\
			&= n\cdot(n-1)\cdot p^{2}
\end{align*}
Plugging this back into $V(X)$, we get
\begin{align*}
	V(X) &= E(X(X-1))+np-n^{2}p^{2}\\
		&= n\cdot(n-1)\cdot p^{2} + np - n^{2}p^{2}\\
		&= np -np^{2}\\
		&= np(1-p)
\end{align*}
\qed\\\\
\textbf{Ex 1:} 3.60 from the textbook\\\\
\textbf{Solution} Fish die with $\bbP$ = 0.2. Therefore the prob of a success (they survive), $\bbP(S)$ = 0.8. Let the random variable $X$ be the number of fish that survive. There are 20 fish. Thus $X \sim$ Bin($n=20$, $p=0.8$). The probability that 14 fish survive is
\[\bbP(X=14) = {20\choose 14}\cdot (0.8)^{14}\cdot(0.2)^{6} \approx 0.1091 = 10.91\%\]
\subsection{Geometric Distribution}
FIrst recall that
\[\sum_{n=0}^{\infty} x^{n} = \frac{1}{1-x}, \  |x|\ \lt\ 1.\]\\
Suppose an experiment leads to either an $S(uccess)$ or an $F(ailure)$. Suppose we want to repeat the experiment until an $S$ occurs\\
Let $\bbP(S) = p$, when 0 \lt $p$ \lt 1, and of course, $\bbP(F) = 1-p$. Then
\[\Om = \{\om_{1}, \om_{2}, \dots, \om_{n}\}\]
where 
\[\om_{n} = FFF\dots FS\]
with $n-1\ Fs$.\\
Let the random variable $X$ reperesent the number of trials needed for as $S$ to occur. Then
\[\Om(X) = \N\]
The probability function of $X$ is then
\[\bbP(X=k) = p\cdot (1-p)^{k-1}\]
If $X$ satisfies this then we write $X \sim$ Geometric($p$).\\\\
\underline{Proposition:}
\begin{enumerate}
	\item $E(X) = \frac{1}{p}$
	\item $V(X) = \frac{1}{p}\cdot (\frac{1}{p} - 1)$
\end{enumerate}
\underline{Proof} of (1): By definition
\begin{align*}
	E(X) &= \sum_{k=1}^{\infty} k\cdot \bbP(X=k)\\
		&= \sum_{k=1}^{\infty} k\cdot p\cdot (1-p)^{k-1}\\
		&= p\cdot \sum_{k=1}^{\infty} k\cdot (1-p)^{k-1}\\
		&= p\cdot \frac{1}{(1-(1-p))^{2}}\\
		&= \frac{p}{p^{2}}\\
		&= \frac{1}{p}
\end{align*}
\qed\\
\underline{Proof} of (2): By definition
\begin{align*}
	V(X) &= E(X^2) - (E(X))^2\\
		&= E(X^2) - \frac{1}{p^2}\\
		&= E(X(X-1)) + E(X) - \frac{1}{p^2}\\
		&= E(X(X-1)) + \frac{1}{p} - \frac{1}{p^2}
\end{align*}
Now we need to reduce down $E(X(X-1))$
\begin{align*}
	E(X(X-1)) &= \sum_{k=1}^{\infty} k\cdot p\cdot (k-1)\cdot (1-p)^{k-1}\\
			&= p\cdot (1-p)\cdot \sum_{k=2}^{\infty} k\cdot (k-1)(1-p)^{k-2}\\
			&= p\x (1-p)\x \frac{2}{p^{3}}\\
			&= \frac{2(1-p)}{p^2}
\end{align*}
Plugging this back into $V(X)$, we get
\begin{align*}
	V(X) &= E(X(X-1)) + \frac{1}{p} - \frac{1}{p^2}\\
		&= \frac{2(1-p)}{p^2} + \frac{1}{p} - \frac{1}{p^2}\\
		&= \frac{2 -2p +1 -1}{p^2}\\
		&= \frac{1-p}{p^2}\\
		&=\frac{1}{p}\x \Big(\frac{1}{p} -1\Big)
\end{align*}
\qed
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 9, 2018}
\subsection{Hypergeometric Distribution}
Suppose we have a population of size $N$, and a subpopulation of size $r$. Each member of the subpopulation has a certain characteristic that the remaining $N - r$ members do not have. Take a sample from the population of size $n$. Then, let $X$ be the random variable containing the number of elements from the subpopulation in the sample.\\\\
\textbf{Ex:} Suppose we draw a 5-card hand out of a deck of 52 cards . Let $X$ be the number of aces in the hand. What are the possible values that $X$ can take on?\\
\textbf{Solution:} We have $N=52$, $n=5$, and $r=4$, and we want to find $X(\Om)$.\\
First, $X \leq n$, and $X \leq r$ $\imply$ $X\leq$ min\{$n, r$\}.\\
Also $n - X \leq n$, and $n-X \leq N-r$ $\imply$ $X \geq 0$, and $X \geq n+r-N$, and therefore $X \geq$ max\{$0, n+r-N$\}.\\
So, $X \in \Z$ such that
\begin{center}
max\{$0, n+r-N$\} $\leq X \leq$ min\{$n, r$\}\\
$\imply$ max\{$0, 9-52$\} $\leq X \leq$ min\{$4, 5$\}\\
$\imply$ 0 $\leq X \leq$ 4
\end{center}
And thus $X \in$ \{0, 1, 2, 3, 4\}, which makes sense.\\\\
\underline{Theorem:} Suppose $X$ is a hypergeometric distribution with parameters $N$, $n$, and $r$. Then
\begin{enumerate}
	\item $E(X) = n\x \frac{r}{N}$
	\item $V(X) = (n\x \frac{r}{N})(1 - \frac{r}{n})(\frac{N-n}{N-1})$
\end{enumerate}
We will prove this later when we're on chapter 5.
\subsubsection{The Probability Function of X}
If $k \in \N_{0}$ such that max\{$0, n+r-N$\} $\leq k \leq$ min\{n, r\}, then
\[p_{X}(k) = \bbP(X=k) = \frac{C_{k}^{r}\x C_{n-k}^{N-r}}{C_{n}^{N}}\]
A random variable $X$ with the above probability function is calles a \ti{hypergeometric distribution} with parameters $N$, $n$, and $r$, and we write 
\begin{center}
$X \sim$ Hypergeometric($N$, $n$, $r$).
\end{center}
\textbf{Ex:} Let $X$ be the number of aces in a 5-card hand drawn from a 52-card deck. Then
\begin{align*}
	p_X(0) &= \frac{C_{0}^{4}\x C_{5}^{48}}{C_{5}^{52}}\\
	p_X(1) &= \frac{C_{1}^{4}\x C_{4}^{48}}{C_{5}^{52}}\\
	p_X(2) &= \frac{C_{2}^{4}\x C_{3}^{48}}{C_{5}^{52}}\\
	\vdots
\end{align*}
\subsection{Poisson Distribution}
Recall if $\la \in \R$ then
\[e^{\la} = \sum_{n=0}^{\infty} \frac{\la^{n}}{n!}\]
Let $\la \geq 0$ be fixed. The function
\[p(n) = e^{-\la}\x \frac{\la^{n}}{n!}\]
for $n \in\N$ is a probability function. This isn't immediately obvious and requires proof.\\
Proof:
\begin{align*}
	\sum_{n=0}^{\infty} p(n) &= e^{-\la}\x \sum_{n=0}^{\infty} \frac{\la^{n}}{n!}\\
						&= e^{-\la}e^{\la}\\
						&= 1
\end{align*}
\qed\\
A random variable $X$ with the above probability function is said to have \ti{poisson distribution} with parameter $\la$. We write $X \sim$ Poisson($\la$).\\\\
\underline{Proposition:} Assume $X \sim$ Poisson($\la$). Then
\begin{enumerate}
	\item $E(X) = \la$
	\item $V(X) = \la$
\end{enumerate}
\underline{Proof} of (1): By definition
\begin{align*}
	E(X) &= \sum_{n=0}^{\infty} n\x p_{x}(n)\\
		&= \sum_{n=1}^{\infty} n\x e^{-\la}\x \frac{\la^{n}}{n!}\\
		&= e^{-\la}\x \sum_{n=1}^{\infty} \frac{\la^{n}}{(n-1)!}\\
		&= \la\x e^{-\la}\x \sum_{n=1}^{\infty} \frac{\la^{n-1}}{(n-1)!}\\
		&= \la\x e^{-\la}\x \sum_{n=0}^{\infty} \frac{\la^{n}}{n!}\\
		&= \la\x e^{-\la}\x e^{\la}\\
		&= \la
\end{align*}
\qed\\
\underline{Proof} of (2): By definition
\begin{align*}
	V(X) &= E(X^{2}) - (E(X))^{2}\\
		&= E(X(X-1)) + E(X) - (E(X))^{2}\\
		&= E(X(X-1)) + \la - \la^{2}\\
		&= \sum_{n=2}^{\infty}[n(n-1)\x p_{X}(n)] +\la - \la^{2}\\
		&= \la^{2} e^{-\la}\x \sum_{n=2}^{\infty} \bigg[\frac{\la^{n-2}}{(n-2)!}\bigg] +\la - \la^{2}\\
		&= \la^{2} e^{-\la}\x \sum_{n=0}^{\infty} \bigg[\frac{\la^{n}}{n!}\bigg] +\la - \la^{2}\\
		&= \la^{2} e^{-\la} e^{\la} +\la - \la^{2}\\
		&= \la^{2} +\la - \la^{2}\\
		&= \la
\end{align*}
\qed\\\\
\textbf{Ex 1:} 3.122 from the book.\\
\textbf{Solution:} Let $X$ be the number of customers at a store. 7 is the mean $\imply$ \\$X \sim$ Poisson($\la =7$).\\
a) $\bbP(X\leq 3)$ = $\sum_{n=0}^{3} p_{X}(n)$ = $\sum_{n=0}^{3} e^{-7}\x \frac{7^{n}}{n!}$\\
b) $\bbP(X\geq 2)$ = $1-\bbP(x\ \lt\ 2) = 1-\bbP(x\leq 1)$ \dots\ plug into probability function. \\\\
\textbf{Ex 2:} Assume that $X_{1} \sim$ Poisson($\la_{1}$), $X_{2} \sim$ Poisson($\la_{2}$). Suppose $X_{1}$, $X_{2}$ are independent events, and \{$X_{1} = n_{1}$\}, \{$X_{2} = n_{2}$\} are independent events for every $n_{1}$, $n_{2}$, $\in \N$. Prove that the distribution given by $X = X_{1} + X_{2}$ is also poisson.\\
\textbf{Solution:} Proceeding by induction on $n$, we have\\
\underline{Base Case:} $n=1$
\begin{align*}
	\bbP(X=1) &= \bbP((X_{1} = 1 \wedge X_{2} = 0)\vee (X_{1} = 0 \wedge X_{2} = 1))\\
			&= \bbP(X_{1} = 1 \wedge X_{2} = 0) + \bbP(X_{1} = 0 \wedge X_{2} = 1)\\
			&= \bbP(X_{1} = 1) \bbP(X_{2} = 0) + \bbP(X_{1} = 0) \bbP(X_{2} = 1)\\
			&= \la_{1}e^{-\la_{1}}e^{-\la_{2}} + \la_{2}e^{-\la_{1}}e^{-\la_{2}}\\
			&= (\la_{1} + \la_{2})e^{-(\la_{1} + \la{2})}
\end{align*}
\begin{flushright}
Q.E.D
\end{flushright}
\underline{Inductive Step:} $n=k$
\begin{align*}
	\bbP(X=k) &= \sum_{i=0}^{k} \bbP(X_{1}=i\ \wedge\ X_{2}=k-i)\\
			&= \sum_{i=0}^{k} \bbP(X_{1}=i)\bbP(X_{2}=k-i)\\
			&= e^{-(\la_{1} + \la_{2})}\x \sum_{i=0}^{k} \frac{\la_{1}^{i}}{i!}\x \frac{\la_{2}^{k-i}}{(k-i)!}\\
			&= \frac{e^{-(\la_{1}+\la_{2})}}{k!}\x \sum_{i=0}^{k} \frac{k!}{i!(k-i)!} \la_{1}^{i}\la_{2}^{k-i}\\
			&= \frac{e^{-(\la_{1}+\la_{2})}}{k!}\x \sum_{i=0}^{k} {k\choose i} \la_{1}^{i}\la_{2}^{k-i}\\
			&= \frac{e^{-(\la_{1}+\la_{2})}}{k!}(\la_{1}+\la_{2})^{k}
\end{align*}
\begin{flushright}
Q.E.D
\end{flushright}
And thus by induction, the distribution of the random variable, formed by summing two random variables with poisson distribution, is itself poisson.\\
\qed\\\\\\
\subsection{Moment Generating Functions}
First we will define \ti{moments of a random variable}. Let $X$ be a random variable. The $n^{th}$ moment of $X$, $\mu_{n}$, is defined as
\[\mu_{n} = E(X^{n})\]
for $n \in \N$.\\\\
\underline{Remark:} If $X$ is  a discrere random variable, then
\[\mu_{n} = E(X^{n}) = \sum_{x} x^{n}\x p_{X}(x)\]\\
Now we'll define \ti{moment generating functions (MGFs)}. Let $X$ be a random variable. The moment generate function of $X$ is defined as
\[m_{X}(t) = E(e^{tX})\]
\underline{Remark:}
\begin{enumerate}
	\item $t=0$ is always in the domain of $m_{X}$
	\item The moment generating function, $m_{X}$, of $X$ is a unique identification of the distribution of $X$, not unlike Laplace transforms serve as a unique identification for differential equation solutions!
\end{enumerate}
\ \\
\textbf{Ex 1:} Consider the probability distribution given by
\begin{center}
\begin{tabular}{| c || c | c | c |}
\hline
$x$ & 0 & -1 & 3\\
\hline
$\bbP(X=x)$ & $\frac{1}{4}$ & $\frac{1}{2}$ &  $\frac{1}{4}$\\
\hline
\end{tabular}
\end{center}
find the moment generating function.\\
\textbf{Solution:} Straight from the definition of the moment generating function, we have
\begin{align*}
	m_{x}(t) &= E(e^{tX}) = \sum_{x} x^{n}\x p_{X}(x)\\
			&= e^{0t}\x p_{X}(0) + e^{-t}\x p_{X}(-1) + e^{3t}\x p_{X}(3)\\
			&= \frac{1}{4}(1+e^{3t}) + \frac{1}{2}e^{-t},\ t \in \R
\end{align*}
\ \\
\textbf{Ex 2:} Let $X$ be a random variable. Consider the moment generating function of $X$, $m_{X}(t)$ given by
\[m_{X}(t) = \frac{1}{2}e^{t} + \frac{1}{6}e^{5t} + \frac{1}{3}e^{-6t}\]
write the probability distribution.\\\\\\
\textbf{Solution:}\\
\begin{center}
\begin{tabular}{| c || c | c | c |}
\hline
$x$ & 1 & 5 & 6\\
\hline
$\bbP(X=x)$ & $\frac{1}{2}$ & $\frac{1}{6}$ & $\frac{1}{3}$\\
\hline
\end{tabular}
\end{center}
It's pretty easy to see the relationship between the equatoin and the table.\\\\
\underline{Remark:}
\begin{align*}
	m_{X}(t) &= E(e^{tX})\\
			&= E\Bigg(\sum_{n=0}^{\infty} \frac{(tX)^{n}}{n!}\Bigg)\\
			&= \sum_{n=0}^{\infty} E\bigg(\frac{(tX)^{n}}{n!}\bigg)\\
			\intertext{(Of course one needs to be careful of doing the last step!)}
	m_{X}(t) &=  \sum_{n=0}^{\infty} \frac{E(X^{x})}{n!}t^{n}
\end{align*}
If that holds on an interval of the form ]$-\ep, \ep$[, then
\[\frac{d^{n}}{dt^{n}}(m_{X}(t)) \bigg\rvert_{t=0} = E(X^{n})\]
We can write this more formally as a theorem.\\\\
\underline{Theorem:} Let $\ep\ \gt\ 0$, and let $X$ be a random variable such that the moment generating function contains an interval of the form ]$-\ep, \ep$[. Then
\[E(X^{n}) = \frac{d^{n}}{dt^{n}}(m_{X}(t))\bigg\rvert_{t=0}\]
We will not prove this right now. ..Maybe later.\\\\\\\\\\
\subsection{The MGF of the Binomial Distribution}
\underline{Proposition:} Let $X$ be a random variable and suppose that $X \sim$ Bin($n, p$). Then
\[m_{X}(t) = (pe^{t} + (1-p))^{n}\]
for $t \in \R$.\\
\underline{Proof:} By definition, we have
\begin{align*}
	m_{X}(t) &= \sum_{k=0}^{n} e^{tk}\x p_{X}(n)\\
	\intertext{Substituting in the probability function for the binomial distribution, we get}
			&= \sum_{k=0}^{n} {n\choose k} e^{tk}p^{k}(1-p)^{n-k}\\
			&= \sum_{k=0}^{n} {n\choose k} (pe^{t})^{k}(1-p)^{n-k}\\
			&= (pe^{t} + (1-p))^{n}, \ t\in \R
\end{align*}
\qed \\\\
\underline{Proposition:} Let $X$ be a random variable and suppose that $X \sim$ Bin($n, p$). Then
\[E(X) = np\]
\underline{Proof:} 
\begin{align*}
	E(X^{n}) &= \frac{d^{n}}{dt^{n}}(m_{X}(t))\bigg\rvert_{t=0}\\
	\intertext{Setting $n=1$}
	E(X) &= \frac{d}{dt}(m_{X}(t))\bigg\rvert_{t=0}\\
		&= \frac{d}{dt}\big[(pe^{t} + (1-p))^{n}\big] \Big\rvert_{t=0}\\
		&= np(pe^{t} + (1-p))^{n-1}\x e^{t} \Big\rvert_{t=0}\\
		&= np
\end{align*}
\qed\\
Note that finding $V(X)$ is just a matter of finding $\frac{d^{2}}{dt^{2}}$.
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 10, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 14, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 16, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 17, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 21, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 22, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 23, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 24, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 28, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 29, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 30, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{May 31, 2018}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}